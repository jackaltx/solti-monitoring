# Alloy Metrics to InfluxDB Implementation Plan

**Created**: 2025-12-30
**Status**: Ready for Implementation
**Effort**: ~2-4 hours
**Risk**: Low (adds observability, doesn't change existing log flow)

## Purpose

Implement metrics collection from Alloy log processing to track:
- Dropped spam/blocklist DNS queries (by service)
- DNS query types (A, AAAA, TXT, etc.)
- Security events (denied, refused queries)
- Zone transfers and notifications

**Goal**: Store metrics in InfluxDB via Telegraf, enabling trend analysis and alerting without storing 9,100+ spam query logs per day.

## Architecture

```
Bind9 logs → Alloy (process + count + drop) → Prometheus metrics endpoint (:12345/metrics)
                                                        ↓
                                                  Telegraf (scraper)
                                                        ↓
                                                     InfluxDB
                                                        ↓
                                                     Grafana
```

## Files to Create/Modify

### 1. Add Variable to Alloy Role Defaults (Modify)

**File**: `roles/alloy/defaults/main.yml`

**Add** (after line 30, after `alloy_filter_cron_noise`):

```yaml
# Metrics and advanced filtering
alloy_bind9_metrics_enabled: false  # Enable metrics generation and spam query dropping (opt-in)
```

**Impact**: Defaults to disabled (safe for multi-workstation deployments), enable in inventory for specific hosts

### 2. Update Bind9 Classifier (Modify)

**File**: `roles/alloy/templates/classifiers/bind9-journal-classifier.alloy.j2`

**Location in file**: After existing query classification logic (~line 50-80)

**Add**:
```jinja2
{% if alloy_bind9_metrics_enabled | default(false) %}
// ====================================================================
// Metrics tracking for DNS queries and dropped spam
// ====================================================================

stage.match {
  selector = '{event_type="query"}'

  // Track all queries by record type (A, AAAA, TXT, etc.)
  stage.regex {
    expression = "query: .* IN (?P<query_type>\\w+)"
  }

  stage.metrics {
    metric.counter {
      name        = "alloy_bind9_queries_by_type_total"
      description = "DNS queries by record type"
      source      = "query_type"
    }
  }

  // Drop localhost spam/blocklist queries with metric tracking
  stage.match {
    selector = '{} |~ "client @\\w+ 127\\.0\\.0\\.1.*(?:rspamd|phishtank|blocklist|dnswl|spamhaus|surbl|senderscore)"'

    stage.regex {
      expression = "(?P<spam_service>rspamd|phishtank|blocklist|dnswl|spamhaus|surbl|senderscore)"
    }

    stage.metrics {
      metric.counter {
        name        = "alloy_bind9_spam_queries_dropped_total"
        description = "Spam/blocklist DNS queries dropped to save storage"
        source      = "spam_service"
      }
    }

    stage.drop {}
  }
}

// Track security events (denied, refused queries)
stage.match {
  selector = '{} |~ "denied|refused"'

  // Extract client IP for security tracking
  stage.regex {
    expression = "client @\\w+ (?P<denied_ip>[\\d\\.]+)"
  }

  stage.metrics {
    metric.counter {
      name        = "alloy_bind9_security_events_total"
      description = "DNS queries denied or refused by ACL"
      source      = "denied_ip"
    }
  }
}

// Track zone transfers
stage.match {
  selector = '{event_type="transfer"}'

  stage.regex {
    expression = "transfer of '(?P<zone>[^']+)'"
  }

  stage.metrics {
    metric.counter {
      name        = "alloy_bind9_zone_transfers_total"
      description = "Zone transfers by zone name"
      source      = "zone"
    }
  }
}
{% endif %}
```

**Impact**:
- Adds ~60 lines to classifier
- Creates 4 new Prometheus metrics
- Drops 66.7% of DNS query logs (spam checks)

### 2. Create Telegraf Input Config (New File)

**File**: `roles/telegraf/templates/inputs/prometheus-alloy.conf.j2`

**Content**:
```jinja2
# ====================================================================
# Scrape Alloy metrics for log processing statistics
# Generated by solti-monitoring/roles/telegraf
# ====================================================================

[[inputs.prometheus]]
  ## Alloy Prometheus metrics endpoint
  urls = ["http://{{ alloy_server_listen_addr | default('127.0.0.1:12345') }}/metrics"]

  ## Scrape interval (how often to collect metrics)
  interval = "60s"

  ## Timeout for HTTP requests
  timeout = "10s"

  ## Metric version (2 = use Prometheus data model)
  metric_version = 2

  ## Only collect Alloy custom metrics (reduces noise)
  ## This filters OUT built-in Alloy metrics we don't need
  fieldpass = [
    "alloy_bind9_queries_by_type_total",
    "alloy_bind9_spam_queries_dropped_total",
    "alloy_bind9_security_events_total",
    "alloy_bind9_zone_transfers_total"
  ]

  ## Add tags to all metrics from this input
  [inputs.prometheus.tags]
    service = "alloy"
    collector = "telegraf"

## Notes:
## - Alloy exposes metrics at http://IP:12345/metrics (Prometheus format)
## - Telegraf converts Prometheus metrics to InfluxDB line protocol
## - Metrics are counters (always increasing), use derivative() in queries
```

**Impact**: New input adds ~100 bytes/minute to InfluxDB (4 metrics × ~25 bytes each)

### 3. Update Telegraf Role Tasks (Modify)

**File**: `roles/telegraf/tasks/main.yml`

**Add** (after existing input deployment tasks, around line 40-60):

```yaml
- name: Deploy Alloy Prometheus scraper input
  template:
    src: inputs/prometheus-alloy.conf.j2
    dest: /etc/telegraf/telegraf.d/prometheus-alloy.conf
    owner: telegraf
    group: telegraf
    mode: '0644'
  notify: restart telegraf
  when:
    - telegraf_scrape_alloy | default(false)
    - telegraf_install_state == 'present'
  tags:
    - telegraf
    - telegraf-config
```

**Impact**: Adds 1 task to Telegraf role, only runs when `telegraf_scrape_alloy: true`

### 4. Add Inventory Variables

**File**: `inventory.yml` (in solti-monitoring collection root)

**Add to fleur host** (around line 20-30 where fleur is defined):

```yaml
fleur.lavnet.net:
  # ... existing vars ...

  # Alloy configuration - SECURITY: localhost only
  alloy_custom_args: "--disable-reporting --server.http.listen-addr=127.0.0.1:12345"

  # Alloy metrics configuration
  alloy_bind9_metrics_enabled: true

  # Telegraf scraping (localhost - same host as Alloy)
  telegraf_scrape_alloy: true
  alloy_server_listen_addr: "127.0.0.1:12345"
```

**Impact**:
- Enables feature on fleur only, no impact on other hosts
- **Security**: Metrics endpoint only accessible from localhost (127.0.0.1), not network
- Telegraf on same host can scrape without network exposure

## Implementation Steps

### Phase 1: Update Templates (15 min)

1. **Edit Bind9 classifier**:
   ```bash
   cd solti-monitoring
   vim roles/alloy/templates/classifiers/bind9-journal-classifier.alloy.j2
   # Add metrics stages from section 1 above
   ```

2. **Create Telegraf input**:
   ```bash
   mkdir -p roles/telegraf/templates/inputs
   vim roles/telegraf/templates/inputs/prometheus-alloy.conf.j2
   # Paste content from section 2 above
   ```

3. **Update Telegraf tasks**:
   ```bash
   vim roles/telegraf/tasks/main.yml
   # Add deployment task from section 3 above
   ```

4. **Update inventory**:
   ```bash
   vim inventory.yml
   # Add variables from section 4 to fleur host
   ```

### Phase 2: Test Locally (15 min)

1. **Validate Alloy config syntax**:
   ```bash
   # Deploy to fleur
   ansible-playbook -i inventory.yml playbooks/deploy-alloy.yml --limit fleur --check

   # If check passes, deploy for real
   ansible-playbook -i inventory.yml playbooks/deploy-alloy.yml --limit fleur
   ```

2. **Verify Alloy metrics endpoint**:
   ```bash
   # SSH to fleur and check metrics (localhost only - no network exposure)
   ssh root@fleur.lavnet.net "curl -s http://127.0.0.1:12345/metrics | grep alloy_bind9"

   # Expected output:
   # alloy_bind9_queries_by_type_total{query_type="A"} 42
   # alloy_bind9_spam_queries_dropped_total{spam_service="rspamd"} 15
   # ...
   ```

3. **Deploy Telegraf**:
   ```bash
   ansible-playbook -i inventory.yml playbooks/deploy-telegraf.yml --limit fleur
   ```

4. **Verify Telegraf is scraping**:
   ```bash
   ssh root@fleur.lavnet.net "journalctl -u telegraf -n 50 | grep -i prometheus"

   # Should see:
   # [inputs.prometheus] ... collected metrics from http://10.10.0.1:12345/metrics
   ```

### Phase 3: Verify in InfluxDB (15 min)

1. **Check data arrival**:
   ```bash
   # Query InfluxDB on monitor11
   ssh root@monitor11.a0a0.org
   influx -database telegraf -execute "SHOW MEASUREMENTS" | grep alloy_bind9

   # Expected:
   # alloy_bind9_queries_by_type_total
   # alloy_bind9_spam_queries_dropped_total
   # alloy_bind9_security_events_total
   # alloy_bind9_zone_transfers_total
   ```

2. **Sample query**:
   ```bash
   influx -database telegraf -execute "
   SELECT derivative(mean(counter), 1h) AS queries_per_hour
   FROM alloy_bind9_spam_queries_dropped_total
   WHERE time > now() - 1h
   GROUP BY spam_service
   "
   ```

3. **Verify data over time** (wait 5 minutes):
   ```bash
   # Should see increasing counter values
   influx -database telegraf -execute "
   SELECT * FROM alloy_bind9_spam_queries_dropped_total
   WHERE time > now() - 5m
   LIMIT 10
   "
   ```

### Phase 4: Create Grafana Dashboard (45 min - optional)

1. **Create new dashboard** "Alloy DNS Processing"

2. **Add panels**:
   - **Spam Queries Dropped Rate** (line chart):
     ```sql
     SELECT derivative(mean("counter"), 1h) AS "per_hour"
     FROM "alloy_bind9_spam_queries_dropped_total"
     WHERE $timeFilter
     GROUP BY time($__interval), "spam_service"
     ```

   - **Query Types Distribution** (stacked area):
     ```sql
     SELECT derivative(mean("counter"), 1h) AS "per_hour"
     FROM "alloy_bind9_queries_by_type_total"
     WHERE $timeFilter
     GROUP BY time($__interval), "query_type"
     ```

   - **Security Events** (stat panel):
     ```sql
     SELECT derivative(mean("counter"), 1h) AS "events_per_hour"
     FROM "alloy_bind9_security_events_total"
     WHERE $timeFilter
     GROUP BY "denied_ip"
     ```

   - **Zone Transfers** (table):
     ```sql
     SELECT derivative(mean("counter"), 24h) AS "transfers_per_day"
     FROM "alloy_bind9_zone_transfers_total"
     WHERE time > now() - 7d
     GROUP BY "zone"
     ```

## Validation Checklist

After implementation, verify:

- [ ] Alloy config deployed successfully (`alloy validate` passes)
- [ ] Alloy metrics endpoint responds: `curl http://10.10.0.1:12345/metrics`
- [ ] Metrics contain `alloy_bind9_*` entries
- [ ] Telegraf config deployed without errors
- [ ] Telegraf logs show successful Prometheus scrapes
- [ ] InfluxDB contains new measurements (4 new tables)
- [ ] Data is accumulating (counter values increasing)
- [ ] Spam queries are being dropped (Loki log count reduced)
- [ ] Security events still appear in Loki (not dropped)

## Expected Outcomes

### Metrics Generated

1. **alloy_bind9_queries_by_type_total**
   - Labels: `query_type` (A, AAAA, TXT, MX, etc.)
   - Rate: ~1,150 increments/2 hours = ~13,800/day
   - Example: `{query_type="A"}` counter=649

2. **alloy_bind9_spam_queries_dropped_total**
   - Labels: `spam_service` (rspamd, blocklist, dnswl, etc.)
   - Rate: ~760 increments/2 hours = ~9,100/day
   - Example: `{spam_service="rspamd"}` counter=145

3. **alloy_bind9_security_events_total**
   - Labels: `denied_ip` (source IP of denied query)
   - Rate: ~80 increments/2 hours = ~960/day
   - Example: `{denied_ip="130.12.182.118"}` counter=15

4. **alloy_bind9_zone_transfers_total**
   - Labels: `zone` (zone name transferred)
   - Rate: ~8 increments/2 hours = ~96/day
   - Example: `{zone="jackaltx.us"}` counter=3

### Storage Impact

**Before**:
- Loki: 1,244 Bind9 entries/2 hours = ~14,928/day
- InfluxDB: 0 Bind9 metrics

**After**:
- Loki: ~485 Bind9 entries/2 hours = ~5,820/day (61% reduction)
- InfluxDB: 4 metrics × 60 data points/hour = ~6KB/day

**Net savings**: ~9,100 log entries/day not stored in Loki

### Query Performance

**Before** (to get spam query count):
```bash
# Query all Loki logs and count in application
curl Loki API | jq | grep rspamd | wc -l
# Slow, processes all logs
```

**After** (query InfluxDB metric):
```sql
SELECT sum(counter) FROM alloy_bind9_spam_queries_dropped_total
WHERE time > now() - 24h
GROUP BY spam_service
```
Fast, pre-aggregated counter

## Rollback Plan

If metrics cause issues:

1. **Disable Telegraf scraping**:
   ```yaml
   # inventory.yml
   telegraf_scrape_alloy: false
   ```
   ```bash
   ansible-playbook playbooks/deploy-telegraf.yml --limit fleur
   ```

2. **Disable Alloy metrics** (keeps spam filtering):
   ```yaml
   # inventory.yml
   alloy_bind9_metrics_enabled: false
   ```
   ```bash
   ansible-playbook playbooks/deploy-alloy.yml --limit fleur
   ```

3. **Remove spam filtering** (back to original):
   - Comment out metrics section in bind9-journal-classifier.alloy.j2
   - Redeploy Alloy

**No data loss**: Loki logs retained, InfluxDB metrics optional

## Future Enhancements

1. **Add metrics for other classifiers**:
   - Mail classifier (spam filtering, delivery success)
   - Apache classifier (requests by type, errors)
   - SSH classifier (auth attempts, failures)

2. **Alerting rules**:
   - Alert on spike in denied queries (potential attack)
   - Alert on unusual zone transfer requests
   - Alert on spam service timeouts

3. **Metric labels**:
   - Add `hostname` label for multi-host deployments
   - Add `zone` label to queries (which zone was queried)

4. **Export to Prometheus**:
   - Configure Alloy to also push to Prometheus
   - Use Prometheus federation for long-term storage

## Reference Documentation

- **Alloy metrics**: https://grafana.com/docs/alloy/latest/reference/components/loki.process/#stagemetrics-block
- **Telegraf Prometheus input**: https://github.com/influxdata/telegraf/tree/master/plugins/inputs/prometheus
- **InfluxDB counter queries**: https://docs.influxdata.com/influxdb/v1/query_language/functions/#derivative

## Related Reports

- **Bind9 Classifier Validation**: `reports/Bind9-Classifier-Validation.md`
- **Loki Filter Efficiency**: `reports/Loki-Report-Process.md`
- **Anonymization Guide**: `articles/ANONYMIZATION_PROCESS.md`

---

## Implementation Timeline

**Recommended Schedule**:
1. Day 1: Update templates and inventory (30 min)
2. Day 1: Deploy to fleur and verify metrics (30 min)
3. Day 1: Deploy Telegraf and verify InfluxDB (30 min)
4. Day 2-7: Monitor for issues, validate data
5. Day 7: Create Grafana dashboards (1 hour)

**Total effort**: ~2-3 hours (excluding monitoring period)

**Dependencies**: None - all components already deployed

**Risk level**: Low
- Additive changes only (no modification to existing log flow)
- Can be disabled via inventory variables
- No impact on other hosts

Ready for implementation when you are!
